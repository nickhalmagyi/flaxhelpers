{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optax Lessons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "\n",
    "\n",
    "seed = 42\n",
    "key = jax.random.PRNGKey(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple linear regression model will let us explore Optax with minimal fuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    features: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.dense = nn.Dense(features=self.features)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.dense(x)\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression(features=1)\n",
    "\n",
    "# Initialize parameters\n",
    "x = jnp.ones((1, 1))  # Dummy input\n",
    "params = model.init(key, x)\n",
    "\n",
    "# Define a simple mean squared error loss function\n",
    "def mse_loss(params, model, x, y):\n",
    "    preds = model.apply(params, x)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "# Example usage\n",
    "x_train = jnp.array([[1.0], [2.0], [3.0], [4.0]])\n",
    "y_train = jnp.array([[2.0], [4.0], [6.0], [8.0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 47.301307678222656\n",
      "Epoch 100, Loss: 0.056781843304634094\n",
      "Epoch 200, Loss: 0.052745550870895386\n",
      "Epoch 300, Loss: 0.049648720771074295\n",
      "Epoch 400, Loss: 0.04675476998090744\n",
      "Epoch 500, Loss: 0.04402981325984001\n",
      "Epoch 600, Loss: 0.04376675933599472\n",
      "Epoch 700, Loss: 0.04350510984659195\n",
      "Epoch 800, Loss: 0.043245431035757065\n",
      "Epoch 900, Loss: 0.04298752173781395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wf/lm962lhn3v30bs2_4s7wd2s80000gn/T/ipykernel_36951/886426542.py:21: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  momentum = jax.tree_map(jnp.zeros_like, params)\n",
      "/var/folders/wf/lm962lhn3v30bs2_4s7wd2s80000gn/T/ipykernel_36951/886426542.py:32: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  momentum_next = jax.tree_map(\n",
      "/var/folders/wf/lm962lhn3v30bs2_4s7wd2s80000gn/T/ipykernel_36951/886426542.py:35: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  param_updates = jax.tree_map(\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import struct\n",
    "import optax\n",
    "\n",
    "\n",
    "def scheduler(epoch: int) -> float:\n",
    "    \"\"\"\"\n",
    "    We use jax.lax.cond as a way to implement conditional logic inside a jitted function.   \n",
    "    \"\"\"\n",
    "    return jax.lax.cond(\n",
    "        epoch < 100,\n",
    "        lambda _: 1e-3,\n",
    "        lambda _: jax.lax.cond(\n",
    "            epoch < 500,\n",
    "            lambda _: 1e-4,\n",
    "            lambda _: 1e-5,\n",
    "            operand=None\n",
    "        ),\n",
    "        operand=None\n",
    "    )\n",
    "\n",
    "# This function will return the optimizer class.\n",
    "def momentum_optimizer(args: dict) -> optax.GradientTransformation:\n",
    "    \"\"\"\n",
    "    Returns a optax.GradientTransformation class. This type of class accepts two functions \n",
    "    and assigns them to the init and update methods.\n",
    "    \"\"\"\n",
    "    beta = args.get('beta', 0.9)\n",
    "    fixed_learning_rate = args.get('learning_rate', None)\n",
    "    scheduler = args.get('scheduler', None)\n",
    "\n",
    "    # to pass objects to jitted functions, we need to define classes as dataclasses\n",
    "    @struct.dataclass\n",
    "    class OptState:\n",
    "        momentum: any\n",
    "\n",
    "    def init_fn(params):\n",
    "        momentum = jax.tree_map(jnp.zeros_like, params)\n",
    "        return OptState(momentum)\n",
    "\n",
    "    def update_fn(grads, opt_state, epoch=None):\n",
    "        # If there is no scheduler, we fall back to the learning rate provided in the args\n",
    "        learning_rate = jax.lax.cond(\n",
    "            scheduler is not None,\n",
    "            lambda _: scheduler(epoch),\n",
    "            lambda _: fixed_learning_rate,\n",
    "            operand=None\n",
    "        )\n",
    "\n",
    "        # update momentum\n",
    "        momentum_next = jax.tree_map(\n",
    "            lambda m, g: beta * m + g, opt_state.momentum, grads\n",
    "        )\n",
    "\n",
    "        # Just compute the update to the params, \n",
    "        # This will be applied outside the optimizer.\n",
    "        param_updates = jax.tree_map(\n",
    "            lambda m: -learning_rate * m, momentum_next\n",
    "        )\n",
    "\n",
    "        return param_updates, OptState(momentum_next)\n",
    "\n",
    "    return optax.GradientTransformation(init_fn, update_fn)\n",
    "\n",
    "# Example usage:\n",
    "opt_args = {'learning_rate': 0.1, 'beta': 0.9, 'scheduler': scheduler}\n",
    "# opt_args = {'learning_rate': 0.1, 'beta': 0.9}\n",
    "optimizer = momentum_optimizer(opt_args)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Training step\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, x, y, epoch):\n",
    "    loss, grads = jax.value_and_grad(mse_loss)(params, model, x, y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, epoch=epoch)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    params, opt_state, loss = train_step(params, opt_state, x_train, y_train, epoch)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natural-grads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
